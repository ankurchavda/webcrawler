# webcrawler

A basic webcrawler made using nodejs.

Dependencies used: 
1. request
2. cheerio
3. url-parse
3. hashmap

Given an initial url and a depth value, the crawler crawls until it has reached to a depth level indicated from the root url.

Note: Values won't be stored in the hashmap. If you desire to retain the values, you might want to store it in the database.
# webcrawler